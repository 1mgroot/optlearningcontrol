{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal control and reinforcement learning with the inverted pendulum\n",
    "\n",
    "the goal of this exercise series is to gain practical experience implementing with value iteration, policy iteration and q-learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few packages we need to import\n",
    "\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import IPython \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a few simple functions to display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_pendulum(x, dt):\n",
    "    \"\"\"\n",
    "    This function makes an animation showing the behavior of the pendulum\n",
    "    takes as input the result of a simulation (with deltaT=0.1s)\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we check if we need to down-sample the data for display\n",
    "    #downsampling (we want 100ms DT or higher)\n",
    "    min_dt = 0.1\n",
    "    if(dt < min_dt):\n",
    "        steps = int(min_dt/dt)\n",
    "        use_dt = int(min_dt * 1000)\n",
    "    else:\n",
    "        steps = 1\n",
    "        use_dt = int(dt * 1000)\n",
    "    plotx = x[:,::steps]\n",
    "    \n",
    "    fig = matplotlib.figure.Figure(figsize=[6,6])\n",
    "    matplotlib.backends.backend_agg.FigureCanvasAgg(fig)\n",
    "    ax = fig.add_subplot(111, autoscale_on=False, xlim=[-1.3,1.3], ylim=[-1.3,1.3])\n",
    "    ax.grid()\n",
    "    \n",
    "    list_of_lines = []\n",
    "    \n",
    "    #create the cart pole\n",
    "    line, = ax.plot([], [], 'k', lw=2)\n",
    "    list_of_lines.append(line)\n",
    "    line, = ax.plot([], [], 'o', lw=2)\n",
    "    list_of_lines.append(line)\n",
    "    \n",
    "    cart_height = 0.25\n",
    "    \n",
    "    def animate(i):\n",
    "        for l in list_of_lines: #reset all lines\n",
    "            l.set_data([],[])\n",
    "        \n",
    "        x_pend = np.sin(plotx[0,i])\n",
    "        y_pend = -np.cos(plotx[0,i])\n",
    "        \n",
    "        list_of_lines[0].set_data([0., x_pend], [0., y_pend])\n",
    "        list_of_lines[1].set_data([x_pend, x_pend], [y_pend, y_pend])\n",
    "        \n",
    "        return list_of_lines\n",
    "    \n",
    "    def init():\n",
    "        return animate(0)\n",
    "\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, np.arange(0, len(plotx[0,:])),\n",
    "        interval=use_dt, blit=True, init_func=init)\n",
    "    plt.close(fig)\n",
    "    plt.close(ani._fig)\n",
    "    IPython.display.display_html(IPython.core.display.HTML(ani.to_html5_video()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(pend, value_function, policy, animate=True):\n",
    "    \"\"\"\n",
    "    This function plots the results. It displays the value function, the policy for all states.\n",
    "    Then it integrates the pendulum from state [0,0] and displays the states and control as a function of time\n",
    "    Finally it shows an animation of the result\n",
    "    \"\"\"\n",
    "    x0 = np.array([0.,0.])\n",
    "\n",
    "    x, u = pend.simulate(x0, policy, 20)\n",
    "\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plt.imshow(value_function.reshape((pend.nq, pend.nv)), extent=[0., 2*np.pi, -pend.v_max, pend.v_max], aspect='auto')\n",
    "    plt.xlabel('Pendulum Angle')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Value Function')\n",
    "\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plt.imshow(policy.reshape((pend.nq, pend.nv)), extent=[0., 2*np.pi, -pend.v_max, pend.v_max], aspect='auto')\n",
    "    plt.xlabel('Pendulum Angle')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Policy')\n",
    "\n",
    "    time = np.linspace(0.,20., len(x[0,:]))\n",
    "    plt.figure()\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(time,x[0,:])\n",
    "    plt.ylabel('angle')\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(time,x[1,:])\n",
    "    plt.ylabel('velocity')\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(time[:-1],u)\n",
    "    plt.ylabel('control')\n",
    "    if animate:\n",
    "        animate_pendulum(x, pend.delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the pendulum\n",
    "\n",
    "Here we define a class that provides functions to work with the inverted pendulum\n",
    "This will be used by the value iteration algorithm and also to test the resulting policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePendulum:\n",
    "    \"\"\"\n",
    "    This class describes a \"discretized\" inverted pendulum and provides some helper functions\n",
    "    to use for value/policy iteration and q-learning with a table\n",
    "    \n",
    "    Rationale: as we will use the pendulum with algorithms using tables, we will handle a\n",
    "    state (i.e. position and velocity of the pendulum) which will be discretized and for every\n",
    "    state we will associate an index that enables to address the table (index is an integer from\n",
    "    0 to number_of_q * number_of_v -1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nq=50, nv=50, nu=3, u_max=5., v_max=6.0):\n",
    "        \"\"\"\n",
    "        constructor of the class, takes as input desired discretization number\n",
    "        nq (for angle), nv (for angular velocity) and nu (for control) and the maximum control\n",
    "        and angular velocity\n",
    "        \"\"\"\n",
    "        #store discretization information\n",
    "        self.nq=nq\n",
    "        self.nv=nv\n",
    "        self.nu = nu\n",
    "        self.v_max = v_max\n",
    "        \n",
    "        # create lookup tables for discretized states\n",
    "        self.u = np.linspace(-u_max, u_max, self.nu)\n",
    "        self.q = np.linspace(0., 2*np.pi, self.nq, endpoint=False)\n",
    "        self.v = np.linspace(-v_max, v_max, self.nv)\n",
    "        \n",
    "        #the total number of discretized states\n",
    "        self.num_states = self.nq * self.nv\n",
    "        \n",
    "        #gravity constant\n",
    "        self.g=9.81\n",
    "\n",
    "        #discretization step\n",
    "        self.delta_t = 0.1\n",
    "        #integration step / smaller than discretization step to ensure stability of integration\n",
    "        self.dt = 0.01\n",
    "        self.integration_ratio = int(self.delta_t/self.dt)\n",
    "        \n",
    "        # we pre-compute every possible transition and store the index of the transition\n",
    "        # in a 2D table (for element address the state and second the control)\n",
    "        self.next_state_index = np.empty([self.num_states, self.nu], dtype=np.int32)\n",
    "        for i in range(self.num_states):\n",
    "            for k in range(self.nu):\n",
    "                x_next = self.step(self.get_states(i), self.u[k])\n",
    "                self.next_state_index[i,k] = self.get_index(x_next)\n",
    "            \n",
    "            \n",
    "    def step(self,x,u):\n",
    "        \"\"\"\n",
    "        This function integrates the pendulum for one step of self.delta_t seconds using\n",
    "        an inner integration step of self.dt (to ensure stable integration)\n",
    "        \n",
    "        Inputs:\n",
    "        x: state of the pendulum (x,v) as a 2D numpy array\n",
    "        u: control as a scalar number\n",
    "        \n",
    "        Output:\n",
    "        the state of the pendulum as a 2D numpy array at the end of the integration\n",
    "        \"\"\"\n",
    "        for i in range(self.integration_ratio):\n",
    "            x_next = (x[0] + self.dt * x[1])%(2*np.pi)\n",
    "            v_next = np.clip(x[1] + self.dt * (u-self.g*np.sin(x[0])), -self.v_max, self.v_max)\n",
    "            x = np.array([x_next,v_next])\n",
    "        return x\n",
    "    \n",
    "    def simulate(self, x0, policy, T):\n",
    "        \"\"\"\n",
    "        This function simulates the pendulum of T seconds from initial state x0 using a discrete policy \n",
    "        \n",
    "        Inputs:\n",
    "        x0: the initial conditions of the pendulum as a 2D array (angle and velocity)\n",
    "        policy: a 1D array containing a discretized policy\n",
    "        T: the time to integrate for\n",
    "        \n",
    "        Output:\n",
    "        x (2D array) and u (1D array) containing the time evolution of states and control\n",
    "        \"\"\"\n",
    "        horizon_length = int(T/self.delta_t)\n",
    "        x=np.empty([2, horizon_length+1])\n",
    "        x[:,0] = x0\n",
    "        u=np.empty([horizon_length])\n",
    "        for i in range(horizon_length):\n",
    "            u[i] = policy[self.get_index(x[:,i])]\n",
    "            x[:,i+1] = self.step(x[:,i], u[i])\n",
    "        return x, u\n",
    "\n",
    "    \n",
    "    def get_index(self, x):\n",
    "        \"\"\"\n",
    "        given an arbitrary 2D state (x) of the pendulum it returns the associated index, \n",
    "        for example to use to address a table\n",
    "        \"\"\"\n",
    "        ind_q = np.argmin((x[0]-self.q)**2)\n",
    "        ind_v = np.argmin((x[1]-self.v)**2)\n",
    "        return ind_q + ind_v*self.nq\n",
    "    \n",
    "    def get_states(self, index):\n",
    "        \"\"\"\n",
    "        given an index, it returns the associated discretized state of the pendulum as a 2D vector\n",
    "        \"\"\"\n",
    "        iv,ix = np.divmod(index, self.nq)\n",
    "        return np.array([self.q[ix], self.v[iv]])                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "This is the instantaneous cost $$g(x,v,u) = 1000(1-\\cos(x-\\pi))^2 + 100 v^2 + u^2$$\n",
    "which gives a high cost for states far from $\\pi$ (i.e. far from the inverted position) or states with non zero velocity or high controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,u):\n",
    "    \"\"\"\n",
    "    a cost function for the inverted pendulum\n",
    "    \"\"\"\n",
    "    return 1000.*(1.-np.cos(x[0]-np.pi))**2 + 100.*x[1]**2 + 1.*u**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "The following class implements the value iteration algorithm as seen in the class. The algorithm is generic and could be used for any model. As a constructor, it gets a dynamic model (that needs to implement the same functions implemented by the pendulum class) and a cost function as define above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    This class is used to implement value iteration and store the state of the value function and policy\n",
    "    as we iterate\n",
    "    \"\"\"\n",
    "    def __init__(self, model, cost, discount_factor=0.99):\n",
    "        \"\"\"\n",
    "        receives as input a pendulum and cost function and potentially a discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # value function stored as a 1D array (indexed as we indexed states in pendulum)\n",
    "        self.value_function = np.zeros([model.num_states])\n",
    "        # we also store the policy similarly\n",
    "        self.policy = np.zeros([model.num_states])\n",
    "        # references to the pendulum and cost function\n",
    "        self.model = model\n",
    "        self.cost = cost\n",
    "        \n",
    "        #discount factor for cost\n",
    "        self.gamma = discount_factor\n",
    "                \n",
    "    def iterate(self, num_iter=1):\n",
    "        \"\"\"\n",
    "        the main iteration of value iteration\n",
    "        num_iter: maximum number of iterations to be performed. \n",
    "        \n",
    "        If after an iteration the value function does not change (e.g. less thant 10e-5)\n",
    "        the function returns and print success\n",
    "        \"\"\"\n",
    "        for i in range(num_iter):\n",
    "            J_new = self.value_function.copy()\n",
    "            for j in range(self.model.num_states):\n",
    "                #for each possible control input we compute the cost\n",
    "                r = np.zeros([self.model.nu])\n",
    "                for l in range(self.model.nu):\n",
    "                    # the current states and control\n",
    "                    x = self.model.get_states(j)\n",
    "                    u = self.model.u[l]\n",
    "                    # the index for the next state\n",
    "                    next_index = self.model.next_state_index[j,l]\n",
    "                    #compute the cost\n",
    "                    r[l] = self.cost(x, u) + self.gamma*self.value_function[next_index]\n",
    "                    \n",
    "                # we take the smallest cost value to update the value function\n",
    "                J_new[j] = np.min(r)\n",
    "                #here we also store the policy (so we have it for later)\n",
    "                self.policy[j] = self.model.u[np.argmin(r)]\n",
    "                \n",
    "            #we update the current value function if there is any change otherwise we are done\n",
    "            if ((self.value_function-J_new)**2 < 10e-5).all():\n",
    "                print(\"CONVERGED after iteration \" + str(i))\n",
    "                break\n",
    "            else:\n",
    "                self.value_function = J_new.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate a pendulum with discretization 50x50 for angle q and velocity v and 3 actions\n",
    "pendulum = DiscretePendulum(nq=50, nv=50, nu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we instanciate a value iteration object for a pendulum model and a cost function\n",
    "value_iteration = ValueIteration(pendulum, cost)\n",
    "\n",
    "# we run the iterations (with maximum number 2000).\n",
    "value_iteration.iterate(2000)\n",
    "\n",
    "# we plot the results\n",
    "plot_results(pendulum, value_iteration.value_function, value_iteration.policy, animate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Answer the question using the value iteration algorithm.\n",
    "1. The algorithm converges in approximately 900 iterations - how many iterations does it take for the algorithm to find a policy that is capable of getting the pendulum upside down but which is not necessarily optimal?\n",
    "2. Change the cost function to give more weight to the cost of control (weight 10, then 100 and then 1000). Analyze how the resulting optimal policy change when changing the control cost. Does the change in cost change the number of iteration necessary for convergence?\n",
    "3. Do the same analysis when changing the weight of the velocity cost (keeping control cost to the original value and vary the velocity cost from 1 to 1000).\n",
    "4. We now use a \"sparse cost\" of the form $$ g(x,v,u)  = \\left\\{ \\begin{matrix} -1 & \\textrm{if |x- $\\pi$ |<0.2} \\\\ 1 & \\textrm{otherwise} \\end{matrix} \\right. $$ where a cost occurs when the pendulum is not close to $\\pi$. Analyze the resulting convergence of the algorithm and the solution obtained compared to the previous costs (include both policy and value functions plots).\n",
    "5. The discretized pendulum contains 3 actions. What happens if 5 actions are used? Compare the solution with the solution found with 3 actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. Implement the Policy Iteration algorithm \n",
    "(Hint: for each policy evaluation step, you may use the previous policy eavluation result as an initial start)\n",
    "2. Compare the policies found using value and policy iteration - are they the same?\n",
    "3. Which algorithm seem to be more efficient? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \"\"\"\n",
    "    Skeleton class to help implement policy iteration (you may use it or write your own solution)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, cost, discount_factor=0.99):\n",
    "        \"\"\"\n",
    "        receives as input a pendulum and cost function and potentially a discount factor\n",
    "        \"\"\"\n",
    "        # we create a table for the value and policy functions\n",
    "        self.value_function = np.zeros([model.num_states])\n",
    "        self.policy = np.zeros([model.num_states])\n",
    "        # we also store the index associated to the policy\n",
    "        self.policy_index = np.zeros([model.num_states], dtype=np.int32)\n",
    "        \n",
    "        self.model = model\n",
    "        self.cost = cost\n",
    "        self.gamma = discount_factor\n",
    "            \n",
    "    def iterate(self, num_iter=1):\n",
    "        \"\"\"\n",
    "        the main iteration of policy iteration\n",
    "        num_iter: maximum number of iterations to be performed. \n",
    "        \n",
    "        If after an iteration the policy does not change (e.g. less thant 10e-5) \n",
    "        the function should return and print success\n",
    "        \"\"\"\n",
    "        for i in range(num_iter):\n",
    "            #policy evaluation\n",
    "            self.policy_evaluation()\n",
    "            #policy update\n",
    "            if not self.policy_update():\n",
    "                print('CONVERGED after iteration ' + str(i))\n",
    "                break\n",
    "            \n",
    "    def policy_update(self):\n",
    "        \"\"\"\n",
    "        TO BE COMPLETED\n",
    "        Policy update function \n",
    "        it should return True if the policy was changed and False otherwise\n",
    "        \"\"\"\n",
    "        return True\n",
    "            \n",
    "    def policy_evaluation(self, num_iter=10000):\n",
    "        \"\"\"\n",
    "        TO BE COMPLETED\n",
    "        Policy evaluation function \n",
    "        \"\"\"             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two exercises are Bonus. Answering these questions will give additional points towards the final grade but it is not necessary to answer them to get the maximum grade.\n",
    "\n",
    "## Exercise 3 [Bonus]\n",
    "1. Implement Q-learning with a table (use episodes of at least 10 seconds and an epsilon greedy policy with $\\epsilon=0.1$).\n",
    "2. How many iterations does it take for the algorithm to learn how to invert the pendulum?\n",
    "3. How can you compute the optimal policy from the Q function? And the optimal value function?\n",
    "4. Plot the learned policy and associated value function and also show these functions at different stages of learning.\n",
    "5. How is learning affected when changing $\\epsilon$ and the learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    \"\"\"\n",
    "    Skeleton class to help implement Q learning with a table (you may use it or write your own solution)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, cost, discount_factor=0.99, learning_rate=0.1, epsilon_greedy=0.1):\n",
    "        # we create tables to store value and policy functions\n",
    "        self.value_function = np.zeros([model.num_states])\n",
    "        self.policy = np.zeros([model.num_states])\n",
    "        \n",
    "        # we create the Q table\n",
    "        self.q_function = np.zeros([model.num_states, model.nu])\n",
    "        \n",
    "        self.model = model\n",
    "        self.cost = cost\n",
    "        \n",
    "        # other parameters\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.gamma = discount_factor\n",
    "        self.alpha = learning_rate\n",
    "            \n",
    "    def iterate(self, num_iter=1):\n",
    "        \"\"\"\n",
    "        TO BE COMPLETED\n",
    "        \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
